{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b46786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pickle, json, glob, time\n",
    "from tqdm import tqdm\n",
    "import pymysql\n",
    "import warnings\n",
    "# import MySQLdb\n",
    "pymysql.install_as_MySQLdb()\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "201feddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본문 크롤링에 필요한 함수를 로딩하고 있습니다...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "검색할 언론사 : 아시아경제\n",
      " query,cd 현대차 005380\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "브라우저를 실행시킵니다(자동 제어)\n",
      "\n",
      "설정한 언론사를 선택합니다.\n",
      "\n",
      "<경제/IT> 카테고리에서 <아시아경제>를 찾았으므로 탐색을 종료합니다\n",
      "\n",
      "크롤링을 시작합니다.\n",
      "  st_n   st_cd   news        date                                    title  \\\n",
      "0  현대차  005380  아시아경제  2020010215  현대차, 작년 전세계서 442만대 판매…\"내년 457만6000대 목표\"   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://view.asiae.co.kr/article/2020010215540...   \n",
      "\n",
      "                                                text  \n",
      "0  국내 74만1842대로 전년 대비 2.9% 증가쏘나타·그랜저, 내수서 나란히 '10...  \n",
      "https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%ED%98%84%EB%8C%80%EC%B0%A8&sort=0&photo=0&field=0&pd=3&ds=20200101&de=20200102&cluster_rank=12&mynews=1&office_type=1&office_section_code=3&news_office_checked=1277&nso=so:r,p:from20200101to20200102,a:all&start=11\n",
      "cureent_nm----- 1\n",
      "cureent_nm----- 2\n",
      "\n",
      "브라우저를 종료합니다.\n",
      "====================================================================================================\n",
      "데이터프레임 변환\n",
      "\n",
      "====================================================================================================\n",
      "결과물의 일부\n"
     ]
    }
   ],
   "source": [
    "########################################## DB에 데이터 넣기 ################################################\n",
    "def insertTOdb(news_df):\n",
    "    \n",
    "\n",
    "    news_df.to_sql(name='news_craw_test', con=engine, if_exists='append',index = False,dtype = {\n",
    "    'st_n':sqlalchemy.types.VARCHAR(10),\n",
    "    'st_cd':sqlalchemy.types.VARCHAR(10),\n",
    "    'news': sqlalchemy.types.TEXT(),\n",
    "    'n_date':sqlalchemy.types.VARCHAR(10),\n",
    "    'title' : sqlalchemy.types.TEXT(), \n",
    "    'url' :sqlalchemy.types.TEXT(),\n",
    "    'text' : sqlalchemy.types.TEXT()\n",
    "\n",
    "    })\n",
    "\n",
    "    \n",
    "################################################################  html 전체 전처리  #####################################\n",
    "def cleanhtml(raw_html):\n",
    "    try:\n",
    "        cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        cleantext = cleantext.replace('\\n','')\n",
    "        cleantext = cleantext.replace('\\t','')\n",
    "        cleantext=cleantext[cleantext.find('마의견'):cleantext.find('재배포 금지')]\n",
    "    except:\n",
    "        cleantext = '결과없음'\n",
    "    print('html 클린')\n",
    "    return cleantext\n",
    "\n",
    "#####\n",
    "################################################# 한 페이지 전체 읽기 ############################################################\n",
    "def one_page_craw(url,query,cd,press_nm):\n",
    "    browser.get(url)\n",
    "    time.sleep(0.07)\n",
    "    df_one_page_craw = pd.DataFrame()\n",
    "    \n",
    "    #페이지 수\n",
    "#     page_ct=len(abc.find_elements_by_xpath('.//a'))\n",
    "#     page_ct_tg = abc.find_elements_by_xpath('.//a')\n",
    "\n",
    "# ####동적 제어로 페이지 넘어가며 크롤링\n",
    "    news_dict = {}\n",
    "    idx = 1\n",
    "    cur_page = 1\n",
    "    table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "    li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n",
    "    news_num = len(li_list)\n",
    "    news_df = pd.DataFrame()\n",
    "\n",
    "    while idx < news_num:\n",
    "        table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "        li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n",
    "        area_list = [li.find_element_by_xpath('.//div[@class=\"news_area\"]') for li in li_list]\n",
    "        a_list = [area.find_element_by_xpath('.//a[@class=\"news_tit\"]') for area in area_list]\n",
    "\n",
    "        for n in a_list[:min(len(a_list), news_num-idx+1)]:\n",
    "            n_url = n.get_attribute('href')\n",
    "\n",
    "            news_sss = {'st_n':query,\n",
    "                              'st_cd':cd,\n",
    "                            'news': press_nm,\n",
    "                            'date':str(crawling_main_text(n_url)[1]),\n",
    "                              'title' : n.get_attribute('title'), \n",
    "                              'url' : n_url,\n",
    "                              'text' : crawling_main_text(n_url)[0]}\n",
    "            idx += 1\n",
    "#             pbar.update(1)\n",
    "\n",
    "            news_df=news_df.append(news_sss,ignore_index=True)\n",
    "            \n",
    "    return news_df\n",
    "\n",
    "\n",
    "\n",
    "########################################### 언론사별 본문 위치 태그 파싱 함수 ############################################\n",
    "print('본문 크롤링에 필요한 함수를 로딩하고 있습니다...\\n' + '-' * 100)\n",
    "def crawling_main_text(url):\n",
    "\n",
    "    req = requests.get(url)\n",
    "    req.encoding = None\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    dates=''\n",
    "    res = []\n",
    "    text=''\n",
    "    # 연합뉴스\n",
    "    if ('://yna' in url) | ('app.yonhapnews' in url): \n",
    "        main_article = soup.find('div', {'class':'story-news article'})\n",
    "        if main_article == None:\n",
    "            main_article = soup.find('div', {'class' : 'article-txt'})\n",
    "            \n",
    "        text = main_article.text\n",
    "        \n",
    "   ###### 전체 html로 수정 ######     \n",
    "   # 매일경제, req.encoding = None 설정 필요\n",
    "    elif 'mk.co' in url:\n",
    "        try:\n",
    "            text = soup.find('div', {'class' : 'art_txt'}).text\n",
    "            text2 = soup.find('li', {'class' : 'lasttime'}).text\n",
    "            dates = re.sub(r'[^0-9]', '', text2)\n",
    "        except:\n",
    "            try:\n",
    "                text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "            except:\n",
    "                text=cleanhtml(str(soup))\n",
    "            try:\n",
    "                text2 = soup.find('li', {'class' : 'lasttime'}).text\n",
    "                dates = re.sub(r'[^0-9]', '', text2)\n",
    "            except:\n",
    "                text2 = '0'\n",
    "#             dates = re.sub(r'[^0-9]', '', text2)\n",
    "\n",
    "    # 매일경제, req.encoding = None 설정 필요\n",
    "    elif 'biz.heraldcorp' in url:\n",
    "        try:\n",
    "            text = soup.find('div',{'class':'article_left'}).text.replace('\\n','')\n",
    "            dates = soup.find('li',{'class':'article_date'}).text.split(' ')[0].replace('.','-')\n",
    "        except:\n",
    "            text = cleanhtml(str(soup))\n",
    "            dates= 0\n",
    "    ## 머니투데이\n",
    "    elif 'biz.heraldcorp' in url:\n",
    "        try:\n",
    "            text = soup.find('div',{'id':'article'}).text.replace('\\n','')\n",
    "            dates = soup.find('li',{'class':'date'}).text.split(' ')[0].replace('.','-')\n",
    "        except:\n",
    "            text = cleanhtml(str(soup))\n",
    "            dates= 0\n",
    "            \n",
    "    ## 아시아\n",
    "    elif 'asiae.co' in url:\n",
    "        try:\n",
    "            text = soup.find('div',{'class':'article fb-quotable'}).text.replace('\\n','')\n",
    "#             dates = soup.find('p',{'class':'user_data'}).text.split('\\t')[0].split(' ')[1:3]\n",
    "#             dates =\" \".join(dates).split(':')[0].replace(' ','').replace('.','')\n",
    "            dates = soup.find('p',{'class':'user_data'}).text\n",
    "            dates = re.sub(r'[^0-9]', '', dates)\n",
    "            dates=dates[:10]\n",
    "        except:\n",
    "            text = cleanhtml(str(soup))\n",
    "            dates= 0\n",
    "    # 그 외\n",
    "    else:\n",
    "        text == None\n",
    "        dates == None\n",
    "    ###### 전체 html로 수정 ######\n",
    "#     try:\n",
    "#         text=text.replace('\\n','').replace('\\r','').replace('<br>','').replace('\\t','')\n",
    "#     except:\n",
    "#         text= None\n",
    "    \n",
    "    res.append(text)\n",
    "    res.append(dates)\n",
    "    return res\n",
    "    \n",
    "    \n",
    "\n",
    "print('검색할 언론사 : {}'.format('아시아경제'))\n",
    "\n",
    "#######################<  실행 파트 >##############################\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "# engine = create_engine(\"mysql+mysqldb://root:\"+\"1234\"+\"@3.35.70.166/proj\", encoding='utf8')\n",
    "engine = create_engine(\"mysql+mysqldb://root:\"+\"1234\"+\"@localhost/proj\", encoding='utf8')\n",
    "conn = engine.connect()\n",
    "#########################################################################################################################\n",
    "################### 브라우저를 켜고 검색 키워드 입력 ###################################################################\n",
    "# querys = {\"삼성전자\":'005930','하이닉스':'000660','네이버':'035420','카카오':'035720','삼성바이오로직스':'207940',\n",
    "#           'LG화학':'051910','현대차':'005380','셀트리온':'068270'}\n",
    "# 'SDI':'006400'\n",
    "# querys= {\"삼성전자\":'005930'}\n",
    "querys= {'현대차':'005380'}\n",
    "# query = input('검색할 키워드  : ')\n",
    "# news_num = int(input('수집 뉴스의 수(숫자만 입력) : '))\n",
    "# 데이터 프레임\n",
    "\n",
    "sleep_sec =0.5\n",
    "news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n",
    "\n",
    "for query,cd in querys.items():\n",
    "    print(' query,cd',query,cd)\n",
    "    # 1 페이지\n",
    "    cureent_nm = 1\n",
    "#     news_num=16\n",
    "#     press_nm = '헤럴드경제'\n",
    "    press_nm = '아시아경제'\n",
    "    print('\\n' + '=' * 100 + '\\n')\n",
    "\n",
    "    print('브라우저를 실행시킵니다(자동 제어)\\n')\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    ############# DevToolsActivePort file doesn't exist error 해결법 #############\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument(\"--single-process\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    chrome_path = './chromedriver.exe'\n",
    "    browser = webdriver.Chrome(chrome_path,chrome_options=chrome_options)\n",
    "\n",
    "    news_url = 'https://search.naver.com/search.naver?where=news&query={}&sm=tab_opt&sort=0&photo=0& \\\n",
    "        field=0&pd=3&ds={}&de={}&start=1'.format(query,20200101,20200102)\n",
    "    browser.get(news_url)\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "\n",
    "    ######### 언론사 선택 및 confirm #####################\n",
    "    print('설정한 언론사를 선택합니다.\\n')\n",
    "\n",
    "#     search_opn_btn = browser.find_element_by_xpath('//a[@class=\"btn_option _search_option_open_btn\"]')\n",
    "#     search_opn_btn.click()\n",
    "#     time.sleep(sleep_sec)\n",
    "\n",
    "    bx_press = browser.find_element_by_xpath('//div[@role=\"listbox\" and @class=\"api_group_option_sort _search_option_detail_wrap\"]//li[@class=\"bx press\"]')\n",
    "\n",
    "    # 기준 두번 째(언론사 분류순) 클릭하고 오픈하기\n",
    "    press_tablist = bx_press.find_elements_by_xpath('.//div[@role=\"tablist\" and @class=\"option\"]/a')\n",
    "    press_tablist[1].click()\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    # 첫 번째 것(언론사 분류선택)\n",
    "    bx_group = bx_press.find_elements_by_xpath('.//div[@class=\"api_select_option type_group _category_select_layer\"]/div[@class=\"select_wrap _root\"]')[0]\n",
    "\n",
    "    press_kind_bx = bx_group.find_elements_by_xpath('.//div[@class=\"group_select _list_root\"]')[0]\n",
    "    press_kind_btn_list = press_kind_bx.find_elements_by_xpath('.//ul[@role=\"tablist\" and @class=\"lst_item _ul\"]/li/a')\n",
    "\n",
    "    \n",
    "###########################  언로사 클릭 ###########################################################################\n",
    "    for press_kind_btn in press_kind_btn_list:\n",
    "\n",
    "        # 언론사 종류를 순차적으로 클릭(좌측)\n",
    "        press_kind_btn.click()\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "        # 언론사선택(우측)\n",
    "        press_slct_bx = bx_group.find_elements_by_xpath('.//div[@class=\"group_select _list_root\"]')[1]\n",
    "        # 언론사 선택할 수 있는 클릭 버튼\n",
    "        press_slct_btn_list = press_slct_bx.find_elements_by_xpath('.//ul[@role=\"tablist\" and @class=\"lst_item _ul\"]/li/a')\n",
    "        # 언론사 이름들 추출\n",
    "        press_slct_btn_list_nm = [psl.text for psl in press_slct_btn_list]\n",
    "\n",
    "        # 언론사 이름 : 언론사 클릭 버튼 인 딕셔너리 생성\n",
    "        press_slct_btn_dict = dict(zip(press_slct_btn_list_nm, press_slct_btn_list))\n",
    "\n",
    "        # 원하는 언론사가 해당 이름 안에 있는 경우\n",
    "        # 1) 클릭하고\n",
    "        # 2) 더이상 언론사분류선택 탐색 중지\n",
    "        if press_nm in press_slct_btn_dict.keys():\n",
    "            print('<{}> 카테고리에서 <{}>를 찾았으므로 탐색을 종료합니다'.format(press_kind_btn.text, press_nm))\n",
    "\n",
    "            press_slct_btn_dict[press_nm].click()\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "            break\n",
    "\n",
    "    ################ 뉴스 크롤링 ########################\n",
    "    \n",
    "    news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n",
    "    print('\\n크롤링을 시작합니다.')\n",
    "    # ####동적 제어로 페이지 넘어가며 크롤링\n",
    "    news_dict = {}\n",
    "    idx = 1\n",
    "    cur_page = 1\n",
    "    \n",
    "    news_dict = {}\n",
    "    idx = 1\n",
    "    cur_page = 1\n",
    "    table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "    li_list = table.find_elements_by_xpath('.//a')\n",
    "    # news_num은 10\n",
    "    news_numss = len(li_list)\n",
    "    abc = browser.find_element_by_xpath('//div[@class=\"sc_page_inner\"]')\n",
    "    \n",
    "    #페이지 수\n",
    "    page_ct=len(abc.find_elements_by_xpath('.//a'))\n",
    "    page_ct_tg = abc.find_elements_by_xpath('.//a')\n",
    "    \n",
    "    ################# 한 페이지 전체 읽기 #######################\n",
    "    urls=browser.current_url\n",
    "    res=one_page_craw(urls,query,cd,press_nm)\n",
    "    news_df=news_df.append(res,ignore_index=True)\n",
    "    print(news_df.head(1))\n",
    "    insertTOdb(news_df)\n",
    "    news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n",
    "    \n",
    "    ############### 다음 페이지 넘기기 ############################\n",
    "    btn_next=browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').get_attribute('href')\n",
    "    print(btn_next)\n",
    "    while btn_next != None:\n",
    "        btn_next=browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').get_attribute('href')\n",
    "#     while page_ct >cureent_nm:\n",
    "        print('cureent_nm-----',cureent_nm)\n",
    "        browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').click()\n",
    "        nxt_pg = browser.current_url\n",
    "#         nxt_pg= browser.current_url+'&start='+str(cureent_nm*10+1)\n",
    "        #### 다음 페이지 이동 ####\n",
    "        browser.get(nxt_pg)\n",
    "        time.sleep(sleep_sec)\n",
    "        urls=browser.current_url\n",
    "        res=one_page_craw(urls,query,cd,press_nm)\n",
    "        news_df=news_df.append(res,ignore_index=True)\n",
    "        cureent_nm += 1\n",
    "        insertTOdb(news_df)\n",
    "        news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n",
    "\n",
    "    ################################################################\n",
    "    else:\n",
    "        print('\\n브라우저를 종료합니다.\\n' + '=' * 100)\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "browser.close()\n",
    "conn.close()\n",
    "#     browser.close()\n",
    "#     break\n",
    "#### 데이터 전처리하기 ###################################################### \n",
    "\n",
    "print('데이터프레임 변환\\n')\n",
    "\n",
    "# news_df = DataFrame(news_dict).T\n",
    "\n",
    "# folder_path = os.getcwd()\n",
    "# xlsx_file_name = '네이버뉴스_본문_{}.xlsx'.format('아시아경제_현대차_2019')\n",
    "\n",
    "# news_df.to_excel(xlsx_file_name)\n",
    "\n",
    "print('=' * 100 + '\\n결과물의 일부')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
