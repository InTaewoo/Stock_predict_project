{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b46786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pickle, json, glob, time\n",
    "from tqdm import tqdm\n",
    "import pymysql\n",
    "import warnings\n",
    "# import MySQLdb\n",
    "pymysql.install_as_MySQLdb()\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201feddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본문 크롤링에 필요한 함수를 로딩하고 있습니다...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "검색할 언론사 : 아시아경제\n",
      " query,cd 셀트리온 068270\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "브라우저를 실행시킵니다(자동 제어)\n",
      "\n",
      "설정한 언론사를 선택합니다.\n",
      "\n",
      "<경제/IT> 카테고리에서 <아시아경제>를 찾았으므로 탐색을 종료합니다\n",
      "\n",
      "크롤링을 시작합니다.\n",
      "   st_n   st_cd   news        date                             title  \\\n",
      "0  셀트리온  068270  아시아경제  2021090307  NS홈쇼핑, 셀트리온 '셀큐어 셀인퓨즈 액티브 크림' 선봬   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://view.asiae.co.kr/article/2021090307581...   \n",
      "\n",
      "                                                text  \n",
      "0  썝蹂몃낫湲 븘씠肄[아시아경제 김유리 기자] NS홈쇼핑은 오는 5일 오후 8시30분 ...  \n",
      "https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%85%80%ED%8A%B8%EB%A6%AC%EC%98%A8&sort=0&photo=0&field=0&pd=3&ds=20200101&de=20210915&cluster_rank=10&mynews=1&office_type=1&office_section_code=3&news_office_checked=1277&nso=so:r,p:from20200101to20210915,a:all&start=11\n",
      "cureent_nm----- 1\n",
      "cureent_nm----- 2\n",
      "cureent_nm----- 3\n",
      "cureent_nm----- 4\n",
      "cureent_nm----- 5\n",
      "cureent_nm----- 6\n",
      "cureent_nm----- 7\n",
      "cureent_nm----- 8\n",
      "cureent_nm----- 9\n",
      "cureent_nm----- 10\n",
      "cureent_nm----- 11\n",
      "cureent_nm----- 12\n",
      "cureent_nm----- 13\n",
      "cureent_nm----- 14\n",
      "cureent_nm----- 15\n",
      "cureent_nm----- 16\n",
      "cureent_nm----- 17\n",
      "cureent_nm----- 18\n",
      "cureent_nm----- 19\n",
      "cureent_nm----- 20\n",
      "cureent_nm----- 21\n",
      "cureent_nm----- 22\n",
      "cureent_nm----- 23\n",
      "cureent_nm----- 24\n",
      "cureent_nm----- 25\n",
      "cureent_nm----- 26\n",
      "cureent_nm----- 27\n",
      "cureent_nm----- 28\n",
      "cureent_nm----- 29\n",
      "cureent_nm----- 30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d0b448031615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0murls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mone_page_craw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpress_nm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0mnews_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mcureent_nm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d0b448031615>\u001b[0m in \u001b[0;36mone_page_craw\u001b[0;34m(url, query, cd, press_nm)\u001b[0m\n\u001b[1;32m     60\u001b[0m                               \u001b[0;34m'st_cd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                             \u001b[0;34m'news'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpress_nm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                             \u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawling_main_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                               \u001b[0;34m'title'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                               \u001b[0;34m'url'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d0b448031615>\u001b[0m in \u001b[0;36mcrawling_main_text\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcrawling_main_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/multi/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################## DB에 데이터 넣기 ################################################\n",
    "def insertTOdb(news_df):\n",
    "    \n",
    "\n",
    "    news_df.to_sql(name='2021_celltrion', con=engine, if_exists='append',index = False,dtype = {\n",
    "    'st_n':sqlalchemy.types.VARCHAR(10),\n",
    "    'st_cd':sqlalchemy.types.VARCHAR(10),\n",
    "    'news': sqlalchemy.types.TEXT(),\n",
    "    'n_date':sqlalchemy.types.VARCHAR(10),\n",
    "    'title' : sqlalchemy.types.TEXT(), \n",
    "    'url' :sqlalchemy.types.TEXT(),\n",
    "    'text' : sqlalchemy.types.TEXT()\n",
    "\n",
    "    })\n",
    "\n",
    "    \n",
    "################################################################  html 전체 전처리  #####################################\n",
    "def cleanhtml(raw_html):\n",
    "    try:\n",
    "        cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        cleantext = cleantext.replace('\\n','')\n",
    "        cleantext = cleantext.replace('\\t','')\n",
    "        cleantext=cleantext[cleantext.find('마의견'):cleantext.find('재배포 금지')]\n",
    "    except:\n",
    "        cleantext = '결과없음'\n",
    "    print('html 클린')\n",
    "    return cleantext\n",
    "\n",
    "#####\n",
    "################################################# 한 페이지 전체 읽기 ############################################################\n",
    "def one_page_craw(url,query,cd,press_nm):\n",
    "    browser.get(url)\n",
    "    time.sleep(0.07)\n",
    "    df_one_page_craw = pd.DataFrame()\n",
    "    \n",
    "    #페이지 수\n",
    "#     page_ct=len(abc.find_elements_by_xpath('.//a'))\n",
    "#     page_ct_tg = abc.find_elements_by_xpath('.//a')\n",
    "\n",
    "# ####동적 제어로 페이지 넘어가며 크롤링\n",
    "    news_dict = {}\n",
    "    idx = 1\n",
    "    cur_page = 1\n",
    "    table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "    li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n",
    "    news_num = len(li_list)\n",
    "    news_df = pd.DataFrame()\n",
    "\n",
    "    while idx < news_num:\n",
    "        table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "        li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n",
    "        area_list = [li.find_element_by_xpath('.//div[@class=\"news_area\"]') for li in li_list]\n",
    "        a_list = [area.find_element_by_xpath('.//a[@class=\"news_tit\"]') for area in area_list]\n",
    "\n",
    "        for n in a_list[:min(len(a_list), news_num-idx+1)]:\n",
    "            n_url = n.get_attribute('href')\n",
    "\n",
    "            news_sss = {'st_n':query,\n",
    "                              'st_cd':cd,\n",
    "                            'news': press_nm,\n",
    "                            'date':str(crawling_main_text(n_url)[1]),\n",
    "                              'title' : n.get_attribute('title'), \n",
    "                              'url' : n_url,\n",
    "                              'text' : crawling_main_text(n_url)[0]}\n",
    "            idx += 1\n",
    "#             pbar.update(1)\n",
    "\n",
    "            news_df=news_df.append(news_sss,ignore_index=True)\n",
    "            \n",
    "    return news_df\n",
    "\n",
    "\n",
    "\n",
    "########################################### 언론사별 본문 위치 태그 파싱 함수 ############################################\n",
    "print('본문 크롤링에 필요한 함수를 로딩하고 있습니다...\\n' + '-' * 100)\n",
    "def crawling_main_text(url):\n",
    "\n",
    "    req = requests.get(url)\n",
    "    req.encoding = None\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    dates=''\n",
    "    res = []\n",
    "    text=''\n",
    "    # 연합뉴스\n",
    "    if ('://yna' in url) | ('app.yonhapnews' in url): \n",
    "        main_article = soup.find('div', {'class':'story-news article'})\n",
    "        if main_article == None:\n",
    "            main_article = soup.find('div', {'class' : 'article-txt'})\n",
    "            \n",
    "        text = main_article.text\n",
    "        \n",
    "   ###### 전체 html로 수정 ######     \n",
    "   # 매일경제, req.encoding = None 설정 필요\n",
    "    elif 'mk.co' in url:\n",
    "        try:\n",
    "            text = soup.find('div', {'class' : 'art_txt'}).text\n",
    "            text2 = soup.find('li', {'class' : 'lasttime'}).text\n",
    "            dates = re.sub(r'[^0-9]', '', text2)\n",
    "        except:\n",
    "            try:\n",
    "                text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "            except:\n",
    "                text=cleanhtml(str(soup))\n",
    "            try:\n",
    "                text2 = soup.find('li', {'class' : 'lasttime'}).text\n",
    "                dates = re.sub(r'[^0-9]', '', text2)\n",
    "            except:\n",
    "                text2 = '0'\n",
    "#             dates = re.sub(r'[^0-9]', '', text2)\n",
    "\n",
    "    # 매일경제, req.encoding = None 설정 필요\n",
    "    elif 'biz.heraldcorp' in url:\n",
    "        try:\n",
    "            text = soup.find('div',{'class':'article_left'}).text.replace('\\n','')\n",
    "            dates = soup.find('li',{'class':'article_date'}).text.split(' ')[0].replace('.','-')\n",
    "        except:\n",
    "            text = cleanhtml(str(soup))\n",
    "            dates= 0\n",
    "    ## 머니투데이\n",
    "    elif 'biz.heraldcorp' in url:\n",
    "        try:\n",
    "            text = soup.find('div',{'id':'article'}).text.replace('\\n','')\n",
    "            dates = soup.find('li',{'class':'date'}).text.split(' ')[0].replace('.','-')\n",
    "        except:\n",
    "            text = cleanhtml(str(soup))\n",
    "            dates= 0\n",
    "            \n",
    "    ## 아시아\n",
    "    elif 'asiae.co' in url:\n",
    "        try:\n",
    "            text = soup.find('div',{'class':'article fb-quotable'}).text.replace('\\n','')\n",
    "#             dates = soup.find('p',{'class':'user_data'}).text.split('\\t')[0].split(' ')[1:3]\n",
    "#             dates =\" \".join(dates).split(':')[0].replace(' ','').replace('.','')\n",
    "            dates = soup.find('p',{'class':'user_data'}).text\n",
    "            dates = re.sub(r'[^0-9]', '', dates)\n",
    "            dates=dates[:10]\n",
    "        except:\n",
    "            text = cleanhtml(str(soup))\n",
    "            dates= 0\n",
    "    # 그 외\n",
    "    else:\n",
    "        text == None\n",
    "        dates == None\n",
    "    ###### 전체 html로 수정 ######\n",
    "#     try:\n",
    "#         text=text.replace('\\n','').replace('\\r','').replace('<br>','').replace('\\t','')\n",
    "#     except:\n",
    "#         text= None\n",
    "    \n",
    "    res.append(text)\n",
    "    res.append(dates)\n",
    "    return res\n",
    "    \n",
    "    \n",
    "\n",
    "print('검색할 언론사 : {}'.format('아시아경제'))\n",
    "\n",
    "#######################<  실행 파트 >##############################\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "# engine = create_engine(\"mysql+mysqldb://root:\"+\"1234\"+\"@3.35.70.166/proj\", encoding='utf8')\n",
    "engine = create_engine(\"mysql+mysqldb://root:\"+\"xodn8050\"+\"@localhost/proj\", encoding='utf8')\n",
    "conn = engine.connect()\n",
    "#########################################################################################################################\n",
    "################### 브라우저를 켜고 검색 키워드 입력 ###################################################################\n",
    "# querys = {\"삼성전자\":'005930','하이닉스':'000660','네이버':'035420','카카오':'035720','삼성바이오로직스':'207940',\n",
    "#           'LG화학':'051910','현대차':'005380','셀트리온':'068270'}\n",
    "# 'SDI':'006400'\n",
    "# querys= {\"삼성전자\":'005930'}\n",
    "querys= {'셀트리온':'068270'}\n",
    "# query = input('검색할 키워드  : ')\n",
    "# news_num = int(input('수집 뉴스의 수(숫자만 입력) : '))\n",
    "# 데이터 프레임\n",
    "\n",
    "sleep_sec =0.5\n",
    "news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n",
    "\n",
    "for query,cd in querys.items():\n",
    "    print(' query,cd',query,cd)\n",
    "    # 1 페이지\n",
    "    cureent_nm = 1\n",
    "#     news_num=16\n",
    "#     press_nm = '헤럴드경제'\n",
    "    press_nm = '아시아경제'\n",
    "    print('\\n' + '=' * 100 + '\\n')\n",
    "\n",
    "    print('브라우저를 실행시킵니다(자동 제어)\\n')\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    ############# DevToolsActivePort file doesn't exist error 해결법 #############\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument(\"--single-process\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    chrome_path = './chromedriver/chromedriver'\n",
    "    browser = webdriver.Chrome(chrome_path,chrome_options=chrome_options)\n",
    "\n",
    "    news_url = 'https://search.naver.com/search.naver?where=news&query={}&sm=tab_opt&sort=0&photo=0& \\\n",
    "        field=0&pd=3&ds={}&de={}&start=1'.format(query,20200101,20210915)\n",
    "    browser.get(news_url)\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "\n",
    "    ######### 언론사 선택 및 confirm #####################\n",
    "    print('설정한 언론사를 선택합니다.\\n')\n",
    "\n",
    "#     search_opn_btn = browser.find_element_by_xpath('//a[@class=\"btn_option _search_option_open_btn\"]')\n",
    "#     search_opn_btn.click()\n",
    "#     time.sleep(sleep_sec)\n",
    "\n",
    "    bx_press = browser.find_element_by_xpath('//div[@role=\"listbox\" and @class=\"api_group_option_sort _search_option_detail_wrap\"]//li[@class=\"bx press\"]')\n",
    "\n",
    "    # 기준 두번 째(언론사 분류순) 클릭하고 오픈하기\n",
    "    press_tablist = bx_press.find_elements_by_xpath('.//div[@role=\"tablist\" and @class=\"option\"]/a')\n",
    "    press_tablist[1].click()\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    # 첫 번째 것(언론사 분류선택)\n",
    "    bx_group = bx_press.find_elements_by_xpath('.//div[@class=\"api_select_option type_group _category_select_layer\"]/div[@class=\"select_wrap _root\"]')[0]\n",
    "\n",
    "    press_kind_bx = bx_group.find_elements_by_xpath('.//div[@class=\"group_select _list_root\"]')[0]\n",
    "    press_kind_btn_list = press_kind_bx.find_elements_by_xpath('.//ul[@role=\"tablist\" and @class=\"lst_item _ul\"]/li/a')\n",
    "\n",
    "    \n",
    "###########################  언로사 클릭 ###########################################################################\n",
    "    for press_kind_btn in press_kind_btn_list:\n",
    "\n",
    "        # 언론사 종류를 순차적으로 클릭(좌측)\n",
    "        press_kind_btn.click()\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "        # 언론사선택(우측)\n",
    "        press_slct_bx = bx_group.find_elements_by_xpath('.//div[@class=\"group_select _list_root\"]')[1]\n",
    "        # 언론사 선택할 수 있는 클릭 버튼\n",
    "        press_slct_btn_list = press_slct_bx.find_elements_by_xpath('.//ul[@role=\"tablist\" and @class=\"lst_item _ul\"]/li/a')\n",
    "        # 언론사 이름들 추출\n",
    "        press_slct_btn_list_nm = [psl.text for psl in press_slct_btn_list]\n",
    "\n",
    "        # 언론사 이름 : 언론사 클릭 버튼 인 딕셔너리 생성\n",
    "        press_slct_btn_dict = dict(zip(press_slct_btn_list_nm, press_slct_btn_list))\n",
    "\n",
    "        # 원하는 언론사가 해당 이름 안에 있는 경우\n",
    "        # 1) 클릭하고\n",
    "        # 2) 더이상 언론사분류선택 탐색 중지\n",
    "        if press_nm in press_slct_btn_dict.keys():\n",
    "            print('<{}> 카테고리에서 <{}>를 찾았으므로 탐색을 종료합니다'.format(press_kind_btn.text, press_nm))\n",
    "\n",
    "            press_slct_btn_dict[press_nm].click()\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "            break\n",
    "\n",
    "    ################ 뉴스 크롤링 ########################\n",
    "    \n",
    "    news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n",
    "    print('\\n크롤링을 시작합니다.')\n",
    "    # ####동적 제어로 페이지 넘어가며 크롤링\n",
    "    news_dict = {}\n",
    "    idx = 1\n",
    "    cur_page = 1\n",
    "    \n",
    "    news_dict = {}\n",
    "    idx = 1\n",
    "    cur_page = 1\n",
    "    table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "    li_list = table.find_elements_by_xpath('.//a')\n",
    "    # news_num은 10\n",
    "    news_numss = len(li_list)\n",
    "    abc = browser.find_element_by_xpath('//div[@class=\"sc_page_inner\"]')\n",
    "    \n",
    "    #페이지 수\n",
    "    page_ct=len(abc.find_elements_by_xpath('.//a'))\n",
    "    page_ct_tg = abc.find_elements_by_xpath('.//a')\n",
    "    \n",
    "    ################# 한 페이지 전체 읽기 #######################\n",
    "    urls=browser.current_url\n",
    "    res=one_page_craw(urls,query,cd,press_nm)\n",
    "    news_df=news_df.append(res,ignore_index=True)\n",
    "    print(news_df.head(1))\n",
    "    insertTOdb(news_df)\n",
    "    news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n",
    "    \n",
    "    ############### 다음 페이지 넘기기 ############################\n",
    "    btn_next=browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').get_attribute('href')\n",
    "    print(btn_next)\n",
    "    while btn_next != None:\n",
    "        btn_next=browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').get_attribute('href')\n",
    "#     while page_ct >cureent_nm:\n",
    "        print('cureent_nm-----',cureent_nm)\n",
    "        browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').click()\n",
    "        nxt_pg = browser.current_url\n",
    "#         nxt_pg= browser.current_url+'&start='+str(cureent_nm*10+1)\n",
    "        #### 다음 페이지 이동 ####\n",
    "        browser.get(nxt_pg)\n",
    "        time.sleep(sleep_sec)\n",
    "        urls=browser.current_url\n",
    "        res=one_page_craw(urls,query,cd,press_nm)\n",
    "        news_df=news_df.append(res,ignore_index=True)\n",
    "        cureent_nm += 1\n",
    "        insertTOdb(news_df)\n",
    "        news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n",
    "\n",
    "    ################################################################\n",
    "    else:\n",
    "        print('\\n브라우저를 종료합니다.\\n' + '=' * 100)\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "browser.close()\n",
    "conn.close()\n",
    "#     browser.close()\n",
    "#     break\n",
    "#### 데이터 전처리하기 ###################################################### \n",
    "\n",
    "print('데이터프레임 변환\\n')\n",
    "\n",
    "# news_df = DataFrame(news_dict).T\n",
    "\n",
    "# folder_path = os.getcwd()\n",
    "# xlsx_file_name = '네이버뉴스_본문_{}.xlsx'.format('아시아경제_현대차_2019')\n",
    "\n",
    "# news_df.to_excel(xlsx_file_name)\n",
    "\n",
    "print('=' * 100 + '\\n결과물의 일부')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a003b-0f9e-4ee5-99e9-c3065ef3ea4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
